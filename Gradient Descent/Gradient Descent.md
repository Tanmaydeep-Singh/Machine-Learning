# Gradient Descent

In mathematics, gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
The idea is to take repeated steps in the opposite direction of the gradient of the function at the current point, because this is the direction of steepest descent



# Cost Function vs Gradient Descent

A **Cost Function** is something we want to minimize. For example, our cost function might be the sum of squared errors over the training set.
**Gradient Descent** is a method for finding the minimum of a function of multiple variables.



# Intuition behind Gradient Descent

Gradient descent is an optimization algorithm thatâ€™s used when training a machine learning model and is based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.
For a start, we have to select a random bias and weights, and then iterate over the slope function to get a slope of 0.


![enter image description here](https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_04-GradientDescent-WHITEBG_0.png)
